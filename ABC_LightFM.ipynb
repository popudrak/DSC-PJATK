{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvHQUfyfNMIP2YrR1lwu0o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/popudrak/DSC-PJATK/blob/main/ABC_LightFM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9K6eoXk3q4vB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from lightfm import LightFM\n",
        "from lightfm.data import Dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ------------------------------------------\n",
        "# Wczytywanie danych\n",
        "# ------------------------------------------\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "test = pd.read_csv(\"test.csv\")\n",
        "metadata_raw = pd.read_csv(\"item_metadata_filtered.csv\")\n",
        "\n",
        "with open(\"id_mappings.json\", \"r\") as f:\n",
        "    id_data = json.load(f)\n",
        "\n",
        "id_map_dict = id_data[\"item_mapping\"]\n",
        "\n",
        "# Konwersja mapy do DataFrame\n",
        "id_map_df = pd.DataFrame({\n",
        "    \"parent_asin\": list(id_map_dict.keys()),\n",
        "    \"mapped_item_id\": list(id_map_dict.values())\n",
        "})\n",
        "\n",
        "# Join metadata z mapą ID\n",
        "metadata_raw['parent_asin'] = metadata_raw['parent_asin'].astype(str)\n",
        "id_map_df['parent_asin'] = id_map_df['parent_asin'].astype(str)\n",
        "metadata = metadata_raw.merge(id_map_df, on='parent_asin', how='left')\n",
        "metadata = metadata[metadata['mapped_item_id'].notnull()].copy()\n",
        "metadata['item_id'] = metadata['mapped_item_id'].astype(int)\n",
        "\n",
        "# ------------------------------------------\n",
        "# Mapowanie użytkowników i produktów na indeksy\n",
        "# ------------------------------------------\n",
        "user_ids = train['user_id'].unique()\n",
        "item_ids = train['item_id'].unique()\n",
        "user_id_map = {uid: i for i, uid in enumerate(user_ids)}\n",
        "item_id_map = {iid: i for i, iid in enumerate(item_ids)}\n",
        "item_id_reverse_map = {i: iid for iid, i in item_id_map.items()}\n",
        "\n",
        "train['user_idx'] = train['user_id'].map(user_id_map)\n",
        "train['item_idx'] = train['item_id'].map(item_id_map)\n",
        "\n",
        "# Mapowanie item_id w metadata na item_idx\n",
        "metadata['item_idx'] = metadata['item_id'].map(item_id_map)\n",
        "metadata = metadata[metadata['item_idx'].notnull()]\n",
        "metadata['item_idx'] = metadata['item_idx'].astype(int)\n",
        "\n",
        "# ------------------------------------------\n",
        "# Przygotowanie cech produktów\n",
        "# ------------------------------------------\n",
        "metadata['main_category'] = metadata['main_category'].fillna(\"Unknown\")\n",
        "metadata['category'] = metadata['category'].fillna(\"Unknown\")\n",
        "metadata['store_missing'] = metadata['store'].isnull()\n",
        "metadata['store'] = metadata['store'].fillna(\"Unknown\")\n",
        "metadata['price_available'] = metadata['price'].notnull()\n",
        "metadata['description_available'] = metadata['description'].apply(lambda x: bool(x and len(x.strip()) > 0))\n",
        "metadata['price_missing'] = metadata['price'].isnull()\n",
        "metadata['price_filled'] = metadata['price'].fillna(-1)\n",
        "metadata['price_bin'] = pd.qcut(metadata.loc[~metadata['price_missing'], 'price'], q=5, labels=False, duplicates='drop')\n",
        "metadata['rating_bin'] = pd.cut(metadata['average_rating'], bins=[0, 2, 3, 4, 5], labels=False)\n",
        "metadata['rating_number_missing'] = metadata['rating_number'].isnull()\n",
        "metadata['rating_number_filled'] = metadata['rating_number'].fillna(-1)\n",
        "metadata['rating_number_log_bin'] = pd.cut(np.log1p(metadata.loc[~metadata['rating_number_missing'], 'rating_number']), bins=5, labels=False)\n",
        "\n",
        "# Popularność produktów\n",
        "item_popularity = train['item_id'].value_counts(normalize=True).to_dict()\n",
        "metadata['popularity_score'] = metadata['item_id'].map(item_popularity).fillna(1e-6)\n",
        "\n",
        "if metadata['popularity_score'].nunique() > 1:\n",
        "    metadata['popularity_bin'] = pd.qcut(metadata['popularity_score'].rank(method='first'), q=5, labels=False, duplicates='drop')\n",
        "else:\n",
        "    metadata['popularity_bin'] = 0\n",
        "\n",
        "add_popularity_feature = True\n",
        "\n",
        "# ------------------------------------------\n",
        "# Lista wszystkich cech itemów\n",
        "# ------------------------------------------\n",
        "item_features_list = (\n",
        "    ['category:' + cat for cat in metadata['main_category'].unique()] +\n",
        "    ['subcategory:' + cat for cat in metadata['category'].unique()] +\n",
        "    ['store:' + store for store in metadata['store'].unique()] +\n",
        "    ['has_images', 'price_available', 'description_available', 'price_missing', 'rating_number_missing', 'store_missing'] +\n",
        "    ['price_bin:' + str(i) for i in range(5)] +\n",
        "    ['rating_bin:' + str(i) for i in range(4)] +\n",
        "    ['rating_number_log_bin:' + str(i) for i in range(5)] +\n",
        "    ['popularity_bin:' + str(i) for i in range(5)]\n",
        ")\n",
        "\n",
        "# ------------------------------------------\n",
        "# Przygotowanie danych dla LightFM\n",
        "# ------------------------------------------\n",
        "dataset = Dataset()\n",
        "dataset.fit(users=user_ids, items=item_ids)\n",
        "dataset.fit_partial(items=item_ids, item_features=item_features_list)\n",
        "\n",
        "(interactions, _) = dataset.build_interactions(\n",
        "    [(row['user_id'], row['item_id'], row['rating']) for _, row in train.iterrows()]\n",
        ")\n",
        "\n",
        "# Funkcja generująca cechy itemów\n",
        "def build_item_features(metadata):\n",
        "    features = []\n",
        "    for _, row in metadata.iterrows():\n",
        "        feats = [\n",
        "            'category:' + row['main_category'],\n",
        "            'subcategory:' + row['category'],\n",
        "            'store:' + row['store']\n",
        "        ]\n",
        "        if row['store_missing']:\n",
        "            feats.append('store_missing')\n",
        "        if row['has_images']:\n",
        "            feats.append('has_images')\n",
        "        if row['price_available']:\n",
        "            feats.append('price_available')\n",
        "        if row['description_available']:\n",
        "            feats.append('description_available')\n",
        "        if row['price_missing']:\n",
        "            feats.append('price_missing')\n",
        "        else:\n",
        "            feats.append(f'price_bin:{int(row[\"price_bin\"])}')\n",
        "        feats.append(f'rating_bin:{int(row[\"rating_bin\"])}')\n",
        "        if row['rating_number_missing']:\n",
        "            feats.append('rating_number_missing')\n",
        "        else:\n",
        "            feats.append(f'rating_number_log_bin:{int(row[\"rating_number_log_bin\"])}')\n",
        "        feats.append(f'popularity_bin:{int(row[\"popularity_bin\"])}')\n",
        "\n",
        "        features.append((row['item_id'], feats))\n",
        "    return features\n",
        "\n",
        "item_features = dataset.build_item_features(build_item_features(metadata))\n",
        "\n",
        "# ------------------------------------------\n",
        "# Trening modelu\n",
        "# ------------------------------------------\n",
        "model = LightFM(loss='warp', no_components=256, item_alpha=1e-6, user_alpha=1e-6, random_state=42)\n",
        "model.fit(interactions, item_features=item_features, epochs=50, num_threads=8)\n",
        "\n",
        "# ------------------------------------------\n",
        "# Predykcja\n",
        "# ------------------------------------------\n",
        "user_seen_items = train.groupby('user_idx')['item_idx'].apply(set).to_dict()\n",
        "all_item_indices = np.arange(len(item_id_map))\n",
        "\n",
        "submission = []\n",
        "user_embeddings, user_biases = model.get_user_representations()\n",
        "global_user_embedding = np.mean(user_embeddings, axis=0)\n",
        "global_user_bias = np.mean(user_biases)\n",
        "item_embeddings, item_biases = model.get_item_representations(features=item_features)\n",
        "cold_scores = item_embeddings.dot(global_user_embedding) + item_biases + global_user_bias\n",
        "\n",
        "item_idx_to_popularity = metadata.set_index('item_idx')['popularity_bin'].to_dict()\n",
        "low_popularity_items = [i for i in range(len(item_id_map)) if item_idx_to_popularity.get(i, 4) <= 2]\n",
        "\n",
        "if len(low_popularity_items) >= 10:\n",
        "    low_pop_scores = cold_scores[low_popularity_items]\n",
        "    top_indices_in_low_pop = np.argsort(-low_pop_scores)[:10]\n",
        "    selected_item_idxs = [low_popularity_items[i] for i in top_indices_in_low_pop]\n",
        "else:\n",
        "    selected_item_idxs = np.argsort(-cold_scores)[:10]\n",
        "\n",
        "cold_top_items = [item_id_reverse_map[i] for i in selected_item_idxs]\n",
        "cold_items_str = ' '.join(map(str, cold_top_items))\n",
        "\n",
        "for uid in tqdm(test['user_id'].values):\n",
        "    if uid not in user_id_map:\n",
        "        submission.append((uid, cold_items_str))\n",
        "        continue\n",
        "\n",
        "    uidx = user_id_map[uid]\n",
        "    seen = user_seen_items.get(uidx, set())\n",
        "    user_ids_array = np.repeat(uidx, len(all_item_indices))\n",
        "\n",
        "    scores = model.predict(user_ids_array, all_item_indices, item_features=item_features)\n",
        "    top_idxs = [i for i in np.argsort(-scores) if i not in seen][:10]\n",
        "    top_items = [item_id_reverse_map[i] for i in top_idxs]\n",
        "\n",
        "    submission.append((uid, ' '.join(map(str, top_items))))\n",
        "\n",
        "submission_df = pd.DataFrame(submission, columns=['user_id', 'predictions'])\n",
        "submission_df.to_csv(\"submission_kaggle.csv\", index=False)"
      ]
    }
  ]
}