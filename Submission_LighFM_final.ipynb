{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPlxctbMW3A7l7N+ZgwGKNK"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install lightfm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tuUPfO5P7iF",
        "outputId": "1cb44827-09ba-48b3-9156-a9a2fe09d68d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lightfm in /usr/local/lib/python3.11/dist-packages (1.17)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from lightfm) (2.0.2)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.11/dist-packages (from lightfm) (1.15.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from lightfm) (2.32.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from lightfm) (1.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->lightfm) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->lightfm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->lightfm) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->lightfm) (2025.6.15)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->lightfm) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->lightfm) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from lightfm import LightFM\n",
        "from lightfm.data import Dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ------------------------------------------\n",
        "# Wczytywanie danych\n",
        "# ------------------------------------------\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "test = pd.read_csv(\"test.csv\")\n",
        "metadata_raw = pd.read_csv(\"item_metadata_filtered.csv\")\n",
        "\n",
        "with open(\"id_mappings.json\", \"r\") as f:\n",
        "    id_data = json.load(f)\n",
        "\n",
        "id_map_dict = id_data[\"item_mapping\"]\n",
        "\n",
        "# Konwersja mapy do DataFrame\n",
        "id_map_df = pd.DataFrame({\n",
        "    \"parent_asin\": list(id_map_dict.keys()),\n",
        "    \"mapped_item_id\": list(id_map_dict.values())\n",
        "})\n",
        "\n",
        "# Join metadata z mapą ID\n",
        "metadata_raw['parent_asin'] = metadata_raw['parent_asin'].astype(str)\n",
        "id_map_df['parent_asin'] = id_map_df['parent_asin'].astype(str)\n",
        "metadata = metadata_raw.merge(id_map_df, on='parent_asin', how='left')\n",
        "metadata = metadata[metadata['mapped_item_id'].notnull()].copy()\n",
        "metadata['item_id'] = metadata['mapped_item_id'].astype(int)\n",
        "\n",
        "# ------------------------------------------\n",
        "# Mapowanie użytkowników i produktów na indeksy\n",
        "# ------------------------------------------\n",
        "user_ids = train['user_id'].unique()\n",
        "item_ids = train['item_id'].unique()\n",
        "user_id_map = {uid: i for i, uid in enumerate(user_ids)}\n",
        "item_id_map = {iid: i for i, iid in enumerate(item_ids)}\n",
        "item_id_reverse_map = {i: iid for iid, i in item_id_map.items()}\n",
        "\n",
        "train['user_idx'] = train['user_id'].map(user_id_map)\n",
        "train['item_idx'] = train['item_id'].map(item_id_map)\n",
        "\n",
        "# Mapowanie item_id w metadata na item_idx\n",
        "metadata['item_idx'] = metadata['item_id'].map(item_id_map)\n",
        "metadata = metadata[metadata['item_idx'].notnull()]\n",
        "metadata['item_idx'] = metadata['item_idx'].astype(int)\n",
        "\n",
        "# ------------------------------------------\n",
        "# Przygotowanie cech produktów\n",
        "# ------------------------------------------\n",
        "metadata['main_category'] = metadata['main_category'].fillna(\"Unknown\")\n",
        "metadata['category'] = metadata['category'].fillna(\"Unknown\")\n",
        "metadata['store_missing'] = metadata['store'].isnull()\n",
        "metadata['store'] = metadata['store'].fillna(\"Unknown\")\n",
        "metadata['price_available'] = metadata['price'].notnull()\n",
        "metadata['description_available'] = metadata['description'].apply(lambda x: bool(x and len(x.strip()) > 0))\n",
        "metadata['price_missing'] = metadata['price'].isnull()\n",
        "metadata['price_filled'] = metadata['price'].fillna(-1)\n",
        "metadata['has_images'] = metadata['image_urls'].notnull()\n",
        "\n",
        "valid_prices = metadata.loc[~metadata['price_missing'], 'price']\n",
        "if valid_prices.nunique() > 1:\n",
        "    metadata.loc[~metadata['price_missing'], 'price_bin'] = pd.qcut(valid_prices, q=5, labels=False, duplicates='drop')\n",
        "else:\n",
        "    metadata['price_bin'] = 0\n",
        "\n",
        "metadata['rating_bin'] = pd.cut(metadata['average_rating'], bins=[0, 2, 3, 4, 5], labels=False).fillna(-1).astype(int)\n",
        "metadata['rating_number_missing'] = metadata['rating_number'].isnull()\n",
        "metadata['rating_number_filled'] = metadata['rating_number'].fillna(-1)\n",
        "metadata['rating_number_log_bin'] = pd.cut(np.log1p(metadata.loc[~metadata['rating_number_missing'], 'rating_number']), bins=5, labels=False)\n",
        "metadata['rating_number_log_bin'] = metadata['rating_number_log_bin'].fillna(-1).astype(int)\n",
        "\n",
        "# Popularność produktów\n",
        "item_popularity = train['item_id'].value_counts(normalize=True).to_dict()\n",
        "metadata['popularity_score'] = metadata['item_id'].map(item_popularity).fillna(1e-6)\n",
        "\n",
        "if metadata['popularity_score'].nunique() > 1:\n",
        "    metadata['popularity_bin'] = pd.qcut(metadata['popularity_score'].rank(method='first'), q=5, labels=False, duplicates='drop')\n",
        "else:\n",
        "    metadata['popularity_bin'] = 0\n",
        "\n",
        "# ------------------------------------------\n",
        "# Lista wszystkich cech itemów\n",
        "# ------------------------------------------\n",
        "item_features_list = (\n",
        "    ['category:' + cat for cat in metadata['main_category'].unique()] +\n",
        "    ['subcategory:' + cat for cat in metadata['category'].unique()] +\n",
        "    ['store:' + store for store in metadata['store'].unique()] +\n",
        "    ['has_images', 'price_available', 'description_available', 'price_missing', 'rating_number_missing', 'store_missing'] +\n",
        "    ['price_bin:' + str(i) for i in range(5)] +\n",
        "    ['rating_bin:' + str(i) for i in range(4)] +\n",
        "    ['rating_number_log_bin:' + str(i) for i in range(5)] +\n",
        "    ['popularity_bin:' + str(i) for i in range(5)]\n",
        ")\n",
        "\n",
        "# ------------------------------------------\n",
        "# Przygotowanie danych dla LightFM\n",
        "# ------------------------------------------\n",
        "dataset = Dataset()\n",
        "dataset.fit(users=user_ids, items=item_ids)\n",
        "dataset.fit_partial(items=item_ids, item_features=item_features_list)\n",
        "\n",
        "(interactions, _) = dataset.build_interactions(\n",
        "    [(row.user_id, row.item_id, row.rating) for row in train.itertuples(index=False)]\n",
        ")\n",
        "\n",
        "# Funkcja generująca cechy itemów\n",
        "def build_item_features(metadata):\n",
        "    features = []\n",
        "    for _, row in metadata.iterrows():\n",
        "        feats = [\n",
        "            'category:' + row['main_category'],\n",
        "            'subcategory:' + row['category'],\n",
        "            'store:' + row['store']\n",
        "        ]\n",
        "        if row['store_missing']:\n",
        "            feats.append('store_missing')\n",
        "        if row['has_images']:\n",
        "            feats.append('has_images')\n",
        "        if row['price_available']:\n",
        "            feats.append('price_available')\n",
        "        if row['description_available']:\n",
        "            feats.append('description_available')\n",
        "        if row['price_missing']:\n",
        "            feats.append('price_missing')\n",
        "        else:\n",
        "            feats.append(f'price_bin:{int(row[\"price_bin\"])}')\n",
        "        feats.append(f'rating_bin:{int(row[\"rating_bin\"])}')\n",
        "        if row['rating_number_missing']:\n",
        "            feats.append('rating_number_missing')\n",
        "        else:\n",
        "            feats.append(f'rating_number_log_bin:{int(row[\"rating_number_log_bin\"])}')\n",
        "        feats.append(f'popularity_bin:{int(row[\"popularity_bin\"])}')\n",
        "\n",
        "        features.append((row['item_id'], feats))\n",
        "    return features\n",
        "\n",
        "item_features = dataset.build_item_features(build_item_features(metadata))\n",
        "\n",
        "# ------------------------------------------\n",
        "# Trening modelu\n",
        "# ------------------------------------------\n",
        "model = LightFM(loss='warp', no_components=128, item_alpha=1e-6, user_alpha=1e-6, random_state=42)\n",
        "model.fit(interactions, item_features=item_features, epochs=30, num_threads=8)"
      ],
      "metadata": {
        "id": "1Ip9mv8Lw2Bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7cd6839-cb55-4578-987c-b0c02402debb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2-3959115706.py:48: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  metadata['item_idx'] = metadata['item_idx'].astype(int)\n",
            "/tmp/ipython-input-2-3959115706.py:53: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  metadata['main_category'] = metadata['main_category'].fillna(\"Unknown\")\n",
            "/tmp/ipython-input-2-3959115706.py:54: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  metadata['category'] = metadata['category'].fillna(\"Unknown\")\n",
            "/tmp/ipython-input-2-3959115706.py:55: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  metadata['store_missing'] = metadata['store'].isnull()\n",
            "/tmp/ipython-input-2-3959115706.py:56: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  metadata['store'] = metadata['store'].fillna(\"Unknown\")\n",
            "/tmp/ipython-input-2-3959115706.py:57: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  metadata['price_available'] = metadata['price'].notnull()\n",
            "/tmp/ipython-input-2-3959115706.py:58: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  metadata['description_available'] = metadata['description'].apply(lambda x: bool(x and len(x.strip()) > 0))\n",
            "/tmp/ipython-input-2-3959115706.py:59: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  metadata['price_missing'] = metadata['price'].isnull()\n",
            "/tmp/ipython-input-2-3959115706.py:60: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  metadata['price_filled'] = metadata['price'].fillna(-1)\n",
            "/tmp/ipython-input-2-3959115706.py:61: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  metadata['has_images'] = metadata['image_urls'].notnull()\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<lightfm.lightfm.LightFM at 0x7d96744c5d50>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------\n",
        "# Predykcja\n",
        "# ------------------------------------------\n",
        "user_seen_items = train.groupby('user_idx')['item_idx'].apply(set).to_dict()\n",
        "all_item_indices = np.arange(len(item_id_map))\n",
        "\n",
        "submission = []\n",
        "user_biases, user_embeddings = model.get_user_representations()\n",
        "global_user_embedding = np.mean(user_embeddings, axis=0)\n",
        "global_user_bias = np.mean(user_biases)\n",
        "item_biases, item_embeddings = model.get_item_representations()\n",
        "cold_scores = item_embeddings.dot(global_user_embedding) + item_biases + global_user_bias\n",
        "score_matrix = user_embeddings.dot(item_embeddings.T) + item_biases\n",
        "test_user_indices = [user_id_map.get(uid, -1) for uid in test['user_id']]\n",
        "\n",
        "item_idx_to_popularity = metadata.set_index('item_idx')['popularity_bin'].to_dict()\n",
        "low_popularity_items = [i for i in range(len(item_id_map)) if item_idx_to_popularity.get(i, 4) <= 2]\n",
        "\n",
        "if len(low_popularity_items) >= 10:\n",
        "    low_pop_scores = cold_scores[low_popularity_items]\n",
        "    top_indices_in_low_pop = np.argsort(-low_pop_scores)[:10]\n",
        "    selected_item_idxs = [low_popularity_items[i] for i in top_indices_in_low_pop]\n",
        "else:\n",
        "    selected_item_idxs = np.argsort(-cold_scores)[:10]\n",
        "\n",
        "cold_top_items = [item_id_reverse_map[i] for i in selected_item_idxs]\n",
        "cold_items_str = ' '.join(map(str, cold_top_items))\n",
        "\n",
        "for uid in tqdm(test['user_id'].values):\n",
        "    if uid not in user_id_map:\n",
        "        submission.append((uid, cold_items_str))\n",
        "        continue\n",
        "\n",
        "    uidx = user_id_map[uid]\n",
        "    seen = user_seen_items.get(uidx, set())\n",
        "    user_ids_array = np.repeat(uidx, len(all_item_indices))\n",
        "\n",
        "    scores = model.predict(user_ids_array, all_item_indices, item_features=item_features)\n",
        "    top_idxs = [i for i in np.argsort(-scores) if i not in seen][:10]\n",
        "    top_items = [item_id_reverse_map[i] for i in top_idxs]\n",
        "\n",
        "    submission.append((uid, ' '.join(map(str, top_items))))\n",
        "\n",
        "submission_df = pd.DataFrame(submission, columns=['user_id', 'predictions'])\n",
        "submission_df.to_csv(\"submission_kaggle.csv\", index=False)"
      ],
      "metadata": {
        "id": "7VA0NN5PHe1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_biases, user_embeddings = model.get_user_representations()\n",
        "item_biases, item_embeddings = model.get_item_representations()\n",
        "\n",
        "# Oblicz macierz score'ów: users x items\n",
        "# (batchowe obliczenia, bardzo szybkie)\n",
        "score_matrix = user_embeddings.dot(item_embeddings.T)  # shape: (n_users, n_items)\n",
        "score_matrix += item_biases  # dodanie biasu itemów\n",
        "score_matrix += user_biases[:, np.newaxis]  # dodanie biasu użytkowników\n",
        "\n",
        "# Przygotowanie słownika widzianych itemów\n",
        "user_seen_items = train.groupby('user_idx')['item_idx'].apply(set).to_dict()\n",
        "all_item_indices = np.arange(len(item_id_map))\n",
        "\n",
        "# Przygotowanie cold start dla nieznanych użytkowników (tak jak wcześniej)\n",
        "global_user_embedding = np.mean(user_embeddings, axis=0)\n",
        "global_user_bias = np.mean(user_biases)\n",
        "cold_scores = item_embeddings.dot(global_user_embedding) + item_biases + global_user_bias\n",
        "\n",
        "item_idx_to_popularity = metadata.set_index('item_idx')['popularity_bin'].to_dict()\n",
        "low_popularity_items = [i for i in range(len(item_id_map)) if item_idx_to_popularity.get(i, 4) <= 2]\n",
        "\n",
        "if len(low_popularity_items) >= 10:\n",
        "    low_pop_scores = cold_scores[low_popularity_items]\n",
        "    top_indices_in_low_pop = np.argsort(-low_pop_scores)[:10]\n",
        "    selected_item_idxs = [low_popularity_items[i] for i in top_indices_in_low_pop]\n",
        "else:\n",
        "    selected_item_idxs = np.argsort(-cold_scores)[:10]\n",
        "\n",
        "cold_top_items = [item_id_reverse_map[i] for i in selected_item_idxs]\n",
        "cold_items_str = ' '.join(map(str, cold_top_items))\n",
        "\n",
        "# ------------------------------------------\n",
        "# Nowa, zoptymalizowana pętla predykcyjna\n",
        "# ------------------------------------------\n",
        "submission = []\n",
        "for uid in tqdm(test['user_id'].values):\n",
        "    uidx = user_id_map.get(uid, -1)\n",
        "    if uidx == -1:\n",
        "        submission.append((uid, cold_items_str))\n",
        "        continue\n",
        "\n",
        "    scores = score_matrix[uidx]\n",
        "    seen = user_seen_items.get(uidx, set())\n",
        "\n",
        "    scores[list(seen)] = -np.inf\n",
        "\n",
        "    top_idxs = np.argpartition(-scores, 10)[:10]\n",
        "    top_idxs = top_idxs[np.argsort(-scores[top_idxs])]\n",
        "    top_items = [item_id_reverse_map[i] for i in top_idxs]\n",
        "\n",
        "    submission.append((uid, ' '.join(map(str, top_items))))\n",
        "\n",
        "submission_df = pd.DataFrame(submission, columns=['user_id', 'predictions'])\n",
        "submission_df.to_csv(\"submission_kaggle.csv\", index=False)"
      ],
      "metadata": {
        "id": "3I-3DDKRIeqf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "6d9a53fc-0973-4441-f597-ee7811d6d0fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-1086431010.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0muser_biases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_user_representations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mitem_biases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_item_representations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Oblicz macierz score'ów: users x items\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# (batchowe obliczenia, bardzo szybkie)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('submission_kaggle.csv')"
      ],
      "metadata": {
        "id": "y5SLbPAuQDug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "submission = pd.read_csv(\"submission_kaggle.csv\", sep=\",\", header=0)\n",
        "print(submission.head())\n",
        "submission['predicted_list'] = submission['predictions'].str.split()\n",
        "submission['unique_count'] = submission['predicted_list'].apply(lambda x: len(set(x)))\n",
        "\n",
        "plt.figure()\n",
        "sns.histplot(submission['unique_count'], bins=10, kde=False)\n",
        "plt.title(\"Liczba unikalnych produktów w rekomendacjach (na użytkownika)\")\n",
        "plt.xlabel(\"Unikalne produkty w TOP-10\")\n",
        "plt.ylabel(\"Liczba użytkowników\")\n",
        "plt.show()\n",
        "\n",
        "# Statystyka: najczęściej rekomendowane produkty\n",
        "top_recommended = pd.Series(np.concatenate(submission['predicted_list'].values)).value_counts().head(20)\n",
        "plt.figure()\n",
        "top_recommended.plot(kind='bar')\n",
        "plt.title(\"Top 20 najczęściej rekomendowanych produktów\")\n",
        "plt.ylabel(\"Liczba rekomendacji\")\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nLiczba unikalnych produktów w predykcjach:\", submission['predicted_list'].explode().nunique())"
      ],
      "metadata": {
        "id": "HnlXKATB-BeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wnioski"
      ],
      "metadata": {
        "id": "eIgcDrM7-GI6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f94Flt-B-H76"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}